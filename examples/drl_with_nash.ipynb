{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício Extra - Quando RL encontra Nash / Quando Nash emerge de RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo deste caderno é implementar um ambiente de ``jogo de par ou impar`` entre um agente e seu ambiente. Um jogo desses em que o ambiente (representando o jogador oponente) é totalmente aleatório poderia não ter tanta graça, pois o agente aleatório se daria tão bem quanto um agente que tenta aprender, e, se o faz direito, aprende que o melhor é ser o agente aleatório.\n",
    "\n",
    "Por essa razão razão a ideia é fazer algo um pouco diferente: um ambiente que joga aleatoriamente mas dando mais ou menos peso para o número par. Esta probabilidade de jogar par, será representada pelo parâmetro $\\alpha = \\mathcal P (\\text{jogar par}) \\in (0, 1)$.\n",
    "\n",
    "A sequência do jogo é:\n",
    "\n",
    "* Primeiro o __ambiente__ escolhe aleatoriamente $\\text{obs} \\in \\{0, 1\\}$ (com $\\beta = \\mathcal P (\\text{pedir par}) \\in (0, 1)$) e envia dentro de uma mensagem para o __agente__. Aqui $\\text{obs} = 0$ significa o que o __ambiente__, que sempre pede qual será o resultado do jogo que dará a vitória (isto é, falou \"par\"). \n",
    "* O __agente__ recebe \\text{obs} e toma uma ação $a \\in \\{0, 1\\}$. Aqui $a = 0$ significa jogar par, e $a = 1$ significa jogar ímpar. \n",
    "* O __ambiente__ recebe a ação do __agente__ $a$, e sorteia um número $b \\in \\{0,1\\}$ (com $\\alpha = \\mathcal P (\\text{jogar par}) \\in (0, 1)$), de modo independente da jogada do __agente__ ou do que foi escolhido em $\\text{obs}$ e soma os dois números.\n",
    "* Observando o resultado da soma e o que ambiente havia cantado do início, decide quem ganhou.\n",
    "* Se o __agente__ ganhou o __ambiente__ dá uma recompensa $r = 1$, caso contrário, dá uma recompensa $r = 0$.\n",
    "* O jogo acaba. Aqui há apenas um episódio. *Obrigado ao Ângelo por notar que como o aprendizado se dá entre episódios, e não dentro do próprio episódio. Por essa razão neste caso $T = 1$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objetivos:\n",
    "1. Ser simples: O ambiente tem que ser simples para conseguir sem implementado de modo fácil.\n",
    "2. Verificar se a política que nossos algoritmos encontram, consegue reproduzir a estratégia ótima em um Equilíbrio de Nash no Jogo Náo-Cooperativo Simultâneo, isto é: se o agente consegue aprender que para maximizar suas chances de vitória deve seguir a estratégia do equilíbrio de Nash.\n",
    "\n",
    "*Simplificação: Para simplificar a análise, na versão 0 do ambiente, ele sempre pedirá par (ou seja, sobre ele agirá uma distribuição discreta degenerada com toda massa em $\\text{obs} = 0$). Isso porque queremos que o equilíbrio de Nash dependa apenas da distribuição de jogo do agente, e não conjuntamente do que ele escolheu (cantou) e da distribuição do agente.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dúvidas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E se eu tivesse dois agentes e um ambiente?\n",
    "Podeira ser o ambiente de um um agente, e vice versa em um jogo sequencial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivação do Equilíbrio do Jogo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui chamaremos de equilíbrio a situação em que o agente dá a melhor resposta possível, considerando o setup do jogo e tendo como referência o ganho esperado do jogo. Nesse sentido permitimos que o agente também seja estocástico, jogando $a = 0$ com probilidade $q \\in [0, 1]$ (no que permitidmos estratégias degeneradas).\n",
    "\n",
    "Isso tem o espírito do Equilíbrio de Nash em Estratégias Mistas em espírito, mas não o é porque no equilíbrio tanto __agente__ quanto __ambiente__ deveriam raciocinar estrategicamente. Seria algo como se o ambiente, também aprendesse com o agente e mudasse o seu parâmetro interno $\\alpha$ para aumentar o seu ganho esperado. Veja que neste caso, se um ganha, necessariamente o outro perde. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Por um momento vamos imaginar que o ambiente também responde aos movimentos do agente (algo como o \"o ambiente não quer perder\"). Neste jogo, o equilíbrio de Nash em Estratégias Puras seria tanto o __agente__ quanto o __ambiente__ jogarem par com probabilidade igual a *0.5*. \n",
    "\n",
    "Prova:\n",
    "\n",
    "Suponha, sem perda de generalidade que o __ambiente__ decide escolher $\\alpha < 1/2$, isso faria com que o __agente__ jogasse ímpar com *100%* de certeza. Mas, o __ambiente__ sabendo disso, se daria conta que em resposta ao __agente__ deveria jogar ímpar com *100%* também. Logo, escolher $\\alpha < 1/2$. não é equilíbrio. O mesmo ocorre para $\\alpha > 1/2$\n",
    "\n",
    "\n",
    ". Escolhemos o conceito de Equilíbrio de Nash porque ele embute a ideia de indicar a melhor resposta de cada agente à cada estratégia do adversário. Aqui melhor resposta entende-se aquela que dá um retorno positivo. \n",
    "\n",
    "O Equilíbrio tem que ser em estratégias mistas, porque o **ambiente** joga um número com *certa probabilidade*. Por hipótese, o **agente** sabe que o seu adversário se comporta assim, sabe que $\\alpha$ é um número fixo, apenas não conhece o valor de $\\alpha$. Desta forma, sua melhor resposta será uma função do valor $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, vamos supor que o __agente__ de referência visa maximizar o ganho esperado no jogo e escolhe $q \\in (0, 1)$ para atingir este objetivo. Vamos derivar qual a sua função de melhor resposta, que dirá com qual probabilidade deverá jogar par (isto é, que dirá qual o $q$ ótimo ($q^*$).\n",
    "\n",
    "Derivação:\n",
    "\n",
    "Se o agente jogar $a = 0$, seu ganho esperado será: $$\n",
    "\n",
    "O ganho esperado do __agente__ no jogo é:\n",
    "\n",
    "$\\begin{align}\n",
    "g_{\\text{agente}}(\\alpha, q) &= q\\cdot (\\alpha\\cdot (0) + (1-\\alpha) \\cdot 1) + (1 - q)\\cdot (\\alpha\\cdot1 + (1 - \\alpha)\\cdot(0)) \\\\\n",
    "&=q\\cdot (1-\\alpha)+ (1 - q)\\cdot \\alpha\n",
    "\\end{align}$\n",
    "\n",
    "O efeito de incrementos de $q$ no ganho esperado é: $$\\frac{\\partial (g_{\\text{agente}})}{\\partial q} (\\alpha, q) = 1 - 2\\alpha$$\n",
    "\n",
    "Desta forma a resposta ótima do agente ideal será:\n",
    "\n",
    "1. Se $\\alpha < 1/2$, então $\\frac{\\partial (g_{\\text{agente}})}{\\partial \\alpha} (\\alpha, q) > 0$ e portanto o __agente__ escolherá $q = 1$ (i.e., sempre vai jogar ímpar).\n",
    "\n",
    "2. Se $\\alpha > 1/2$, então $\\frac{\\partial (g_{\\text{agente}})}{\\partial \\alpha} (\\alpha, q) < 0$ e portanto o __agente__ escolherá $q = 0$ (i.e., sempre vai jogar par).\n",
    "\n",
    "3. Se $\\alpha = 1/2$, então $g_{\\text{agente}}(\\alpha, q) = 1/2$ e portanto o __agente__ escolherá qualquer $q \\in [0, 1]$.\n",
    "\n",
    "\n",
    "__Atenção__: O resultado esperado de teoria dos jogos com Equilíbrio de Nash de Estratégias Mistas é de que no caso (3) o agente jogará $q=1/2$, e não que ele será indiferente. Isso ocorre porque no arcabouço de teoria dos jogos os seguintes raciocínios se seguirão:\n",
    "\n",
    "* O __ambiente__ (que neste exemplo será estratégico, tanto quanto o __agente__) pensará:\n",
    "\n",
    "1. Se o __agente__ escolher $q \\neq 1/2$ e $q \\in (0,1)$ ele vai me incentivar a sempre jogar uma das opções com certeza (pois o __ambiente__, tem uma função de ganho esperado similar a do __agente__). \n",
    "1.1 Mas se eu jogar uma delas com certeza, o __agente__ vai querer jogar, também, uma estratégia com certeza (ou seja vai querer escolher $q \\neq 1$ ou $q \\neq 0$. Logo, a ideia inicial de jogar algo diferente de $1/2$ e em $(0, 1)$ não é ótima.\n",
    "2. Se o __agente__ escolher $q = 0$ ou $q = 1$, ele vai incentivar o __ambiente__ jogar uma delas com certeza.\n",
    "2.1 Mas, se o __ambiente__ responde dessa forma, é ótimo para o __agente__ mudar de posição, isto é, se escolheu $q = 0$, vai querer mudar (após esse raciocínio) para $q = 1$. Por sua vez, se escolheu $q = 1$ vai querer mudar (após esse raciocínio) para $q = 0$, logo a decisão inicial em 2. não era ótima.\n",
    "3. Portanto, o agente nunca escolherá (após raciocinar as reações do __ambiente__) $q \\neq 1/2$ e $q \\neq (0,1)$ ou $q = 0$ ou $q = 1$. Ou seja, escolherá apenas $q = 1/2$.\n",
    "\n",
    "> Portanto, em equilíbrio os dois jogam $q = \\alpha = 1/2$ e ninguém tem incentivo de se desviar. \n",
    "\n",
    "> Resumo: Se o __agente__ souber que o __ambiente__ é estratégico e é racional (na definição de racional de equilíbrio de Nash) ele não será mais indiferente entre escolher qualquer valor para $q$. *É pelo __ambiente__ ser estático que o agente fica indiferente no caso em que $\\alpha = 1/2$*.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
